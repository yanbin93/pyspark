{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[2]\").appName('kmeans').enableHiveSupport().getOrCreate()\n",
    "conf = SparkConf().setAppName(\"kmeansExample\").setMaster(\"local[2]\")\n",
    "sc = spark.sparkContext\n",
    "from pyspark.ml.feature import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|values|features|\n",
      "+------+--------+\n",
      "|   0.5|     0.0|\n",
      "+------+--------+\n",
      "\n",
      "+------+-----+\n",
      "|values|freqs|\n",
      "+------+-----+\n",
      "|   0.5|  0.0|\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+\n",
      "|values|freqs|\n",
      "+------+-----+\n",
      "|   0.5|  0.0|\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#通过设置阈值，二分类特征\n",
    "df = spark.createDataFrame([(0.5,)], [\"values\"])\n",
    "binarizer = Binarizer(threshold=1.0, inputCol=\"values\", outputCol=\"features\")\n",
    "binarizer.transform(df).show()\n",
    "binarizer.setParams(outputCol='freqs').transform(df).show()\n",
    "binarizer.setOutputCol('freqs').transform(df).show()\n",
    "binarizer.getThreshold()\n",
    "# binarizerPath = temp_path + \"/binarizer\"\n",
    "# binarizer.save(binarizerPath)\n",
    "# loadedBinarizer = Binarizer.load(binarizerPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|values|\n",
      "+------+\n",
      "|   0.1|\n",
      "|   0.4|\n",
      "|   1.2|\n",
      "|   1.5|\n",
      "|   NaN|\n",
      "|   NaN|\n",
      "+------+\n",
      "\n",
      "+------+-------+\n",
      "|values|buckets|\n",
      "+------+-------+\n",
      "|   0.1|    0.0|\n",
      "|   0.4|    0.0|\n",
      "|   1.2|    1.0|\n",
      "|   1.5|    2.0|\n",
      "|   NaN|    3.0|\n",
      "|   NaN|    3.0|\n",
      "+------+-------+\n",
      "\n",
      "[-inf, 0.5, 1.4, inf]\n"
     ]
    }
   ],
   "source": [
    "#Bucketizer，离散化函数\n",
    "values = [(0.1,), (0.4,), (1.2,), (1.5,), (float(\"nan\"),), (float(\"nan\"),)]\n",
    "# print type(values[0])\n",
    "df = spark.createDataFrame(values,[\"values\"])\n",
    "df.show()\n",
    "bucketizer = Bucketizer(splits=[-float(\"inf\"), 0.5, 1.4, float(\"inf\")],inputCol=\"values\",outputCol=\"buckets\")\n",
    "bucketed = bucketizer.setHandleInvalid(\"keep\").transform(df)\n",
    "bucketed.show()\n",
    "# bucketizerPath = temp_path + \"/bucketizer\"\n",
    "# bucketizer.save(bucketizerPath)\n",
    "# loadedBucketizer = Bucketizer.load(bucketizerPath)\n",
    "# loadedBucketizer.getSplits() == bucketizer.getSplits()\n",
    "print bucketizer.getSplits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+----------------+\n",
      "|          features|label|selectedFeatures|\n",
      "+------------------+-----+----------------+\n",
      "|[0.0,0.0,18.0,1.0]|  1.0|      [18.0,1.0]|\n",
      "|[0.0,1.0,12.0,0.0]|  0.0|      [12.0,0.0]|\n",
      "|[1.0,0.0,15.0,0.1]|  0.0|      [15.0,0.1]|\n",
      "+------------------+-----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#卡方选择器\n",
    "# Chi-Squared feature selection, which selects categorical features to use for predicting a categorical label.\n",
    "from pyspark.ml.linalg import Vectors\n",
    "df = spark.createDataFrame(\\\n",
    "    [(Vectors.dense([0.0, 0.0, 18.0, 1.0]), 1.0),\\\n",
    "     (Vectors.dense([0.0, 1.0, 12.0, 0.0]), 0.0),\\\n",
    "     (Vectors.dense([1.0, 0.0, 15.0, 0.1]), 0.0)],\\\n",
    "    [\"features\", \"label\"])\n",
    "selector = ChiSqSelector(numTopFeatures=2, outputCol=\"selectedFeatures\")\n",
    "model = selector.fit(df)\n",
    "model.transform(df).show()\n",
    "# chiSqSelectorPath = temp_path + \"/chi-sq-selector\"\n",
    "# selector.save(chiSqSelectorPath)\n",
    "# loadedSelector = ChiSqSelector.load(chiSqSelectorPath)\n",
    "# loadedSelector.getNumTopFeatures() == selector.getNumTopFeatures()\n",
    "# modelPath = temp_path + \"/chi-sq-selector-model\"\n",
    "# model.save(modelPath)\n",
    "# loadedModel = ChiSqSelectorModel.load(modelPath)\n",
    "# loadedModel.selectedFeatures == model.selectedFeatures\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+-------------------------------+\n",
      "|label|raw             |vectors                        |\n",
      "+-----+----------------+-------------------------------+\n",
      "|0    |[a, b, c]       |(4,[0,1,2],[1.0,1.0,1.0])      |\n",
      "|1    |[ab, b, b, c, a]|(4,[0,1,2,3],[2.0,1.0,1.0,1.0])|\n",
      "+-----+----------------+-------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 词频统计\n",
    "# class pyspark.ml.feature.CountVectorizer(self, minTF=1.0, minDF=1.0, vocabSize=1 << 18, binary=False, inputCol=None, outputCol=None)\n",
    "# Extracts a vocabulary from document collections and generates a CountVectorizerModel.\n",
    "df = spark.createDataFrame(\\\n",
    "     [(0, [\"a\", \"b\", \"c\"]), (1, [\"ab\", \"b\", \"b\", \"c\", \"a\"])],\\\n",
    "     [\"label\", \"raw\"])\n",
    "cv = CountVectorizer(inputCol=\"raw\", outputCol=\"vectors\")\n",
    "model = cv.fit(df)\n",
    "model.transform(df).show(truncate=False)\n",
    "sorted(model.vocabulary) == ['a','ab', 'b', 'c']\n",
    "# countVectorizerPath = temp_path + \"/count-vectorizer\"\n",
    "# cv.save(countVectorizerPath)\n",
    "# loadedCv = CountVectorizer.load(countVectorizerPath)\n",
    "# loadedCv.getMinDF() == cv.getMinDF()\n",
    "# loadedCv.getMinTF() == cv.getMinTF()\n",
    "# loadedCv.getVocabSize() == cv.getVocabSize()\n",
    "# modelPath = temp_path + \"/count-vectorizer-model\"\n",
    "# model.save(modelPath)\n",
    "# loadedModel = CountVectorizerModel.load(modelPath)\n",
    "# loadedModel.vocabulary == model.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([5.0, 8.0, 6.0])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DCT 变换\n",
    "# class pyspark.ml.feature.DCT(self, inverse=False, inputCol=None, outputCol=None)\n",
    "# A feature transformer that takes the 1D discrete cosine transform of a real vector.\n",
    "# No zero padding is performed on the input vector. It returns a real vector of the same length representing the DCT. \n",
    "# The return vector is scaled such that the transform matrix is unitary (aka scaled DCT-II).\n",
    "from pyspark.ml.linalg import Vectors\n",
    "df1 = spark.createDataFrame([(Vectors.dense([5.0, 8.0, 6.0]),)], [\"vec\"])\n",
    "dct = DCT(inverse=False, inputCol=\"vec\", outputCol=\"resultVec\")\n",
    "df2 = dct.transform(df1)\n",
    "df2.head().resultVec\n",
    "df3 = DCT(inverse=True, inputCol=\"resultVec\", outputCol=\"origVec\").transform(df2)\n",
    "df3.head().origVec\n",
    "# dctPath = temp_path + \"/dct\"\n",
    "# dct.save(dctPath)\n",
    "# loadedDtc = DCT.load(dctPath)\n",
    "# loadedDtc.getInverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([4.0, 3.0, 15.0])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#点乘\n",
    "from pyspark.ml.linalg import Vectors\n",
    "df = spark.createDataFrame([(Vectors.dense([2.0, 1.0, 3.0]),)], [\"values\"])\n",
    "ep = ElementwiseProduct(scalingVec=Vectors.dense([1.0, 2.0, 3.0]),\\\n",
    "    inputCol=\"values\", outputCol=\"eprod\")\n",
    "ep.transform(df).head().eprod\n",
    "ep.setParams(scalingVec=Vectors.dense([2.0, 3.0, 5.0])).transform(df).head().eprod\n",
    "# elementwiseProductPath = temp_path + \"/elementwise-product\"\n",
    "# ep.save(elementwiseProductPath)\n",
    "# loadedEp = ElementwiseProduct.load(elementwiseProductPath)\n",
    "# loadedEp.getScalingVec() == ep.getScalingVec()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(5, {0: 1.0, 1: 1.0, 2: 1.0})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\"TF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。\n",
    "# 字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。\n",
    "df = spark.createDataFrame([([\"a\", \"b\", \"c\"],)], [\"words\"])\n",
    "hashingTF = HashingTF(numFeatures=10, inputCol=\"words\", outputCol=\"features\")\n",
    "hashingTF.transform(df).head().features\n",
    "hashingTF.setParams(outputCol=\"freqs\").transform(df).head().freqs\n",
    "params = {hashingTF.numFeatures: 5, hashingTF.outputCol: \"vector\"}\n",
    "hashingTF.transform(df, params).head().vector\n",
    "# hashingTFPath = temp_path + \"/hashing-tf\"\n",
    "# hashingTF.save(hashingTFPath)\n",
    "# loadedHashingTF = HashingTF.load(hashingTFPath)\n",
    "# loadedHashingTF.getNumFeatures() == hashingTF.getNumFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|       tf|      idf|\n",
      "+---------+---------+\n",
      "|[1.0,2.0]|[0.0,0.0]|\n",
      "|[0.0,1.0]|[0.0,0.0]|\n",
      "|[3.0,0.2]|[0.0,0.0]|\n",
      "+---------+---------+\n",
      "\n",
      "+---------+--------------------+\n",
      "|       tf|              vector|\n",
      "+---------+--------------------+\n",
      "|[1.0,2.0]|[0.28768207245178...|\n",
      "|[0.0,1.0]|           [0.0,0.0]|\n",
      "|[3.0,0.2]|[0.86304621735534...|\n",
      "+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "df = spark.createDataFrame([(DenseVector([1.0, 2.0]),),\\\n",
    "(DenseVector([0.0, 1.0]),), (DenseVector([3.0, 0.2]),)], [\"tf\"])\n",
    "idf = IDF(minDocFreq=3, inputCol=\"tf\", outputCol=\"idf\")\n",
    "model = idf.fit(df)\n",
    "model.idf\n",
    "model.transform(df).show()\n",
    "idf.setParams(outputCol=\"freqs\").fit(df).transform(df).collect()[1].freqs\n",
    "params = {idf.minDocFreq: 1, idf.outputCol: \"vector\"}\n",
    "idf.fit(df, params).transform(df).show()\n",
    "# idfPath = temp_path + \"/idf\"\n",
    "# idf.save(idfPath)\n",
    "# loadedIdf = IDF.load(idfPath)\n",
    "# loadedIdf.getMinDocFreq() == idf.getMinDocFreq()\n",
    "# modelPath = temp_path + \"/idf-model\"\n",
    "# model.save(modelPath)\n",
    "# loadedModel = IDFModel.load(modelPath)\n",
    "# loadedModel.transform(df).head().idf == model.transform(df).head().idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-57-8cfa3311a22b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-57-8cfa3311a22b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    class pyspark.ml.feature.MaxAbsScaler(self, inputCol=None, outputCol=None)[source]¶\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class pyspark.ml.feature.MaxAbsScaler(self, inputCol=None, outputCol=None)[source]¶\n",
    "class pyspark.ml.feature.MaxAbsScalerModel(java_model=None)[source]¶\n",
    "class pyspark.ml.feature.MinMaxScaler(self, min=0.0, max=1.0, inputCol=None, outputCol=None)[source]¶\n",
    "class pyspark.ml.feature.NGram(self, n=2, inputCol=None, outputCol=None)[source]¶\n",
    "class pyspark.ml.feature.Normalizer(self, p=2.0, inputCol=None, outputCol=None)[source]\n",
    "class pyspark.ml.feature.OneHotEncoder(self, includeFirst=True, inputCol=None, outputCol=None)[source]\n",
    "class pyspark.ml.feature.PCA(self, k=None, inputCol=None, outputCol=None)[source]¶\n",
    "class pyspark.ml.feature.PolynomialExpansion(self, degree=2, inputCol=None, outputCol=None)[source]\n",
    "class pyspark.ml.feature.QuantileDiscretizer(self, numBuckets=2, inputCol=None, outputCol=None, relativeError=0.001, \n",
    "class pyspark.ml.feature.RegexTokenizer(self, minTokenLength=1, gaps=True, pattern=\"s+\", inputCol=None, outputCol=None, toLowercase=True)[source]¶\n",
    "class pyspark.ml.feature.RFormula(self, formula=None, featuresCol=\"features\", labelCol=\"label\", forceIndexLabel=False)                                            \n",
    "class pyspark.ml.feature.RFormulaModel(java_model=None)[source]\n",
    "class pyspark.ml.feature.StandardScaler(self, withMean=False, withStd=True, inputCol=None, outputCol=None)[source]¶                                             \n",
    "class pyspark.ml.feature.StopWordsRemover(self, inputCol=None, outputCol=None, stopWords=None, caseSensitive=false)[source]                        \n",
    "class pyspark.ml.feature.StringIndexer(self, inputCol=None, outputCol=None, handleInvalid=\"error\")[source]\n",
    "class pyspark.ml.feature.Tokenizer(self, inputCol=None, outputCol=None)[source]\n",
    "class pyspark.ml.feature.VectorAssembler(self, inputCols=None, outputCol=None)[source]¶\n",
    "class pyspark.ml.feature.VectorIndexer(self, maxCategories=20, inputCol=None, outputCol=None)[source]¶\n",
    "class pyspark.ml.feature.VectorSlicer(self, inputCol=None, outputCol=None, indices=None, names=None)[source]\n",
    "class pyspark.ml.feature.Word2Vec(self, vectorSize=100, minCount=5, numPartitions=1, stepSize=0.025, maxIter=1, seed=None, inputCol=None, outputCol=None, windowSize=5, maxSentenceLength=1000)[source]\n",
    "                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
