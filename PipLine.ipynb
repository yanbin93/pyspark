{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----+\n",
      "| id|            text|label|\n",
      "+---+----------------+-----+\n",
      "|  0| a b c d e spark|  1.0|\n",
      "|  1|             b d|  0.0|\n",
      "|  2|    spark f g h |  1.0|\n",
      "|  3|hadoop mapreduct|  0.0|\n",
      "+---+----------------+-----+\n",
      "\n",
      "+---+-------------+----------------+--------------------+--------------------+--------------------+----------+\n",
      "| id|         text|           words|            features|       rawPrediction|         probability|prediction|\n",
      "+---+-------------+----------------+--------------------+--------------------+--------------------+----------+\n",
      "|  4|  spark i j k|[spark, i, j, k]|(1000,[105,149,32...|[0.16293291377589...|[0.54064335448523...|       0.0|\n",
      "|  5|        l m n|       [l, m, n]|(1000,[6,638,655]...|[2.64074492868042...|[0.93343826273835...|       0.0|\n",
      "|  6|      spark a|      [spark, a]|(1000,[105,170],[...|[-1.7313553283508...|[0.15041430048073...|       1.0|\n",
      "|  7|apache hadoop|[apache, hadoop]|(1000,[181,495],[...|[3.74294051364969...|[0.97686361395183...|       0.0|\n",
      "+---+-------------+----------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext,SparkConf\n",
    "spark = SparkSession.builder.\\\n",
    "            master(\"local\").\\\n",
    "            appName(\"my App Name\").\\\n",
    "            getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline,PipelineModel\n",
    "\n",
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.sql import Row\n",
    "training = spark.createDataFrame([(0,\"a b c d e spark\",1.0,),\\\n",
    "                                (1,\"b d\",0.0,),(2,\"spark f g h \",1.0,),\\\n",
    "                                (3, \"hadoop mapreduct\",0.0,)],[\"id\",\"text\",\"label\"])\n",
    "training.show()\n",
    "tokenizer = Tokenizer(inputCol=\"text\",outputCol=\"words\",)\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(),outputCol=\"features\",numFeatures=1000)\n",
    "lr = LogisticRegression().setMaxIter(10).setRegParam(0.01)\n",
    "pipeline = Pipeline(stages=[tokenizer,hashingTF,lr])\n",
    "model = pipeline.fit(training)\n",
    "test = spark.createDataFrame([(4,\"spark i j k\"),(5,\"l m n\"),(6,\"spark a\"),(7,\"apache hadoop\")],[\"id\",\"text\"])\n",
    "model.transform(test).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
