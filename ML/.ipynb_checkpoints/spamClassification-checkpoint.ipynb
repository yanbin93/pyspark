{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SQLContext,SparkSession\n",
    "from pyspark.ml.feature import Tokenizer,HashingTF,IDF\n",
    "conf = SparkConf().setMaster('local')\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"pyspark\") \\\n",
    "    .config(conf = conf) \\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=SparseVector(2000, {240: 0.6931, 333: 0.6931, 1105: 1.3863, 1329: 0.5754, 1357: 0.6931, 1777: 0.6931}), words=[u'i', u'heard', u'about', u'spark', u'and', u'i', u'love', u'spark']),\n",
       " Row(features=SparseVector(2000, {213: 0.6931, 342: 0.6931, 489: 0.6931, 495: 0.6931, 1329: 0.2877, 1809: 0.6931, 1967: 0.6931}), words=[u'i', u'wish', u'java', u'could', u'use', u'case', u'classes']),\n",
       " Row(features=SparseVector(2000, {286: 0.6931, 695: 0.6931, 1138: 0.6931, 1193: 0.6931, 1604: 0.6931}), words=[u'logistic', u'regression', u'models', u'are', u'neat'])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "sentenceData = spark.createDataFrame([(0, \"I heard about Spark and I love Spark\"),\\\n",
    "                                     (0, \"I wish Java could use case classes\"),\\\n",
    "                                     (1, \"Logistic regression models are neat\")],[\"label\",\"sentence\"])\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\",outputCol=\"words\")\n",
    "hashingTf =HashingTF(numFeatures=2000,inputCol=\"words\",outputCol=\"rawFeatures\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "featureizedData = hashingTf.transform(wordsData)\n",
    "idf = IDF(inputCol='rawFeatures',outputCol='features')\n",
    "idfModel = idf.fit(featureizedData)\n",
    "rescaledData = idfModel.transform(featureizedData)\n",
    "# featureizedData.select('rawFeatures','label','words').head()\n",
    "rescaledData.select('features','words').take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:0.69314718056\n",
      "heard:0.69314718056\n",
      "about:1.38629436112\n",
      "spark:0.575364144904\n",
      "and:0.69314718056\n",
      "i:0.69314718056\n",
      "i:0.69314718056\n",
      "wish:0.69314718056\n",
      "java:0.69314718056\n",
      "could:0.69314718056\n",
      "use:0.287682072452\n",
      "case:0.69314718056\n",
      "classes:0.69314718056\n",
      "logistic:0.69314718056\n",
      "regression:0.69314718056\n",
      "models:0.69314718056\n",
      "are:0.69314718056\n",
      "neat:0.69314718056\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer,hashingTf,idfModel])\n",
    "pipelineModel = pipeline.fit('sentenceData')\n",
    "rescaledData = pipelineModel.transform(sentenceData)\n",
    "for i in pipelineModel.transform(sentenceData).select('features','words').take(3):\n",
    "    for j in zip(i[1],filter(lambda x:x!=0,list(i[0].toArray()))):\n",
    "        print \":\".join([str(x) for x in j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o21.partitions.\n: java.net.ConnectException: Call From master1/127.0.0.1 to master1:9002 failed on connection exception: java.net.ConnectException: 拒绝连接; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1473)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1400)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)\n\tat com.sun.proxy.$Proxy20.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:752)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy21.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1977)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:1118)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:1114)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1114)\n\tat org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:57)\n\tat org.apache.hadoop.fs.Globber.glob(Globber.java:252)\n\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1644)\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:257)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:202)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.ConnectException: 拒绝连接\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:608)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:706)\n\tat org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:369)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1522)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1439)\n\t... 45 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d7a8574c1aa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mpositiveExamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspamFeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLabeledPoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mnegativeExamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhamFeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLabeledPoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpositiveExamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegativeExamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mtraining_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Cache data since Logistic Regression is an iterative algorithm.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yanbin/hadoop-2.6/spark/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36munion\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    562\u001b[0m                       self.ctx.serializer)\n\u001b[1;32m    563\u001b[0m         if (self.partitioner == other.partitioner and\n\u001b[0;32m--> 564\u001b[0;31m                 self.getNumPartitions() == rdd.getNumPartitions()):\n\u001b[0m\u001b[1;32m    565\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitioner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yanbin/hadoop-2.6/spark/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2424\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prev_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2426\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yanbin/hadoop-2.6/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yanbin/hadoop-2.6/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31m<type 'str'>\u001b[0m: (<type 'exceptions.UnicodeEncodeError'>, UnicodeEncodeError('ascii', u'An error occurred while calling o21.partitions.\\n: java.net.ConnectException: Call From master1/127.0.0.1 to master1:9002 failed on connection exception: java.net.ConnectException: \\u62d2\\u7edd\\u8fde\\u63a5; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\\n\\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\\n\\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\\n\\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\\n\\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)\\n\\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)\\n\\tat org.apache.hadoop.ipc.Client.call(Client.java:1473)\\n\\tat org.apache.hadoop.ipc.Client.call(Client.java:1400)\\n\\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)\\n\\tat com.sun.proxy.$Proxy20.getFileInfo(Unknown Source)\\n\\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:752)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\\n\\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\\n\\tat com.sun.proxy.$Proxy21.getFileInfo(Unknown Source)\\n\\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1977)\\n\\tat org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:1118)\\n\\tat org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:1114)\\n\\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\\n\\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1114)\\n\\tat org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:57)\\n\\tat org.apache.hadoop.fs.Globber.glob(Globber.java:252)\\n\\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1644)\\n\\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:257)\\n\\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)\\n\\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)\\n\\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:202)\\n\\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\\n\\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\\n\\tat scala.Option.getOrElse(Option.scala:121)\\n\\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\\n\\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\\n\\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)\\n\\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)\\n\\tat scala.Option.getOrElse(Option.scala:121)\\n\\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:250)\\n\\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:61)\\n\\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\\n\\tat py4j.Gateway.invoke(Gateway.java:280)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\\n\\tat java.lang.Thread.run(Thread.java:745)\\nCaused by: java.net.ConnectException: \\u62d2\\u7edd\\u8fde\\u63a5\\n\\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\\n\\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\\n\\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\\n\\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)\\n\\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)\\n\\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:608)\\n\\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:706)\\n\\tat org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:369)\\n\\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1522)\\n\\tat org.apache.hadoop.ipc.Client.call(Client.java:1439)\\n\\t... 45 more\\n', 180, 184, 'ordinal not in range(128)'))"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sc = SparkContext(appName=\"PythonBookExample\")\n",
    "\n",
    "    # Load 2 types of emails from text files: spam and ham (non-spam).\n",
    "    # Each line has text from one email.\n",
    "    spam = sc.textFile(\"files/spam.txt\")\n",
    "    ham = sc.textFile(\"files/ham.txt\")\n",
    "\n",
    "    # Create a HashingTF instance to map email text to vectors of 100 features.\n",
    "    tf = HashingTF(numFeatures = 100)\n",
    "    # Each email is split into words, and each word is mapped to one feature.\n",
    "    spamFeatures = spam.map(lambda email: tf.transform(email.split(\" \")))\n",
    "    hamFeatures = ham.map(lambda email: tf.transform(email.split(\" \")))\n",
    "\n",
    "    # Create LabeledPoint datasets for positive (spam) and negative (ham) examples.\n",
    "    positiveExamples = spamFeatures.map(lambda features: LabeledPoint(1, features))\n",
    "    negativeExamples = hamFeatures.map(lambda features: LabeledPoint(0, features))\n",
    "    training_data = positiveExamples.union(negativeExamples)\n",
    "    training_data.cache() # Cache data since Logistic Regression is an iterative algorithm.\n",
    "\n",
    "    # Run Logistic Regression using the SGD optimizer.\n",
    "    # regParam is model regularization, which can make models more robust.\n",
    "    model = LogisticRegressionWithSGD.train(training_data)\n",
    "\n",
    "    # Test on a positive example (spam) and a negative one (ham).\n",
    "    # First apply the same HashingTF feature transformation used on the training data.\n",
    "    posTestExample = tf.transform(\"O M G GET cheap stuff by sending money to ...\".split(\" \"))\n",
    "    negTestExample = tf.transform(\"Hi Dad, I started studying Spark the other ...\".split(\" \"))\n",
    "\n",
    "    # Now use the learned model to predict spam/ham for new emails.\n",
    "    print \"Prediction for positive test example: %g\" % model.predict(posTestExample)\n",
    "    print \"Prediction for negative test example: %g\" % model.predict(negTestExample)\n",
    "\n",
    "    sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
