{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SQLContext,SparkSession\n",
    "from pyspark.ml.feature import Tokenizer,HashingTF,IDF\n",
    "conf = SparkConf().setMaster('local')\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"pyspark\") \\\n",
    "    .config(conf = conf) \\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=SparseVector(2000, {240: 0.6931, 333: 0.6931, 1105: 1.3863, 1329: 0.5754, 1357: 0.6931, 1777: 0.6931}), words=[u'i', u'heard', u'about', u'spark', u'and', u'i', u'love', u'spark']),\n",
       " Row(features=SparseVector(2000, {213: 0.6931, 342: 0.6931, 489: 0.6931, 495: 0.6931, 1329: 0.2877, 1809: 0.6931, 1967: 0.6931}), words=[u'i', u'wish', u'java', u'could', u'use', u'case', u'classes']),\n",
       " Row(features=SparseVector(2000, {286: 0.6931, 695: 0.6931, 1138: 0.6931, 1193: 0.6931, 1604: 0.6931}), words=[u'logistic', u'regression', u'models', u'are', u'neat'])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "sentenceData = spark.createDataFrame([(0, \"I heard about Spark and I love Spark\"),\\\n",
    "                                     (0, \"I wish Java could use case classes\"),\\\n",
    "                                     (1, \"Logistic regression models are neat\")],[\"label\",\"sentence\"])\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\",outputCol=\"words\")\n",
    "hashingTf =HashingTF(numFeatures=2000,inputCol=\"words\",outputCol=\"rawFeatures\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "featureizedData = hashingTf.transform(wordsData)\n",
    "idf = IDF(inputCol='rawFeatures',outputCol='features')\n",
    "idfModel = idf.fit(featureizedData)\n",
    "rescaledData = idfModel.transform(featureizedData)\n",
    "# featureizedData.select('rawFeatures','label','words').head()\n",
    "rescaledData.select('features','words').take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:0.69314718056\n",
      "heard:0.69314718056\n",
      "about:1.38629436112\n",
      "spark:0.575364144904\n",
      "and:0.69314718056\n",
      "i:0.69314718056\n",
      "i:0.69314718056\n",
      "wish:0.69314718056\n",
      "java:0.69314718056\n",
      "could:0.69314718056\n",
      "use:0.287682072452\n",
      "case:0.69314718056\n",
      "classes:0.69314718056\n",
      "logistic:0.69314718056\n",
      "regression:0.69314718056\n",
      "models:0.69314718056\n",
      "are:0.69314718056\n",
      "neat:0.69314718056\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=[tokenizer,hashingTf,idfModel])\n",
    "pipelineModel = pipeline.fit('sentenceData')\n",
    "rescaledData = pipelineModel.transform(sentenceData)\n",
    "for i in pipelineModel.transform(sentenceData).select('features','words').take(3):\n",
    "    for j in zip(i[1],filter(lambda x:x!=0,list(i[0].toArray()))):\n",
    "        print \":\".join([str(x) for x in j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sc = SparkContext(appName=\"PythonBookExample\")\n",
    "\n",
    "    # Load 2 types of emails from text files: spam and ham (non-spam).\n",
    "    # Each line has text from one email.\n",
    "    spam = sc.textFile(\"files/spam.txt\")\n",
    "    ham = sc.textFile(\"files/ham.txt\")\n",
    "\n",
    "    # Create a HashingTF instance to map email text to vectors of 100 features.\n",
    "    tf = HashingTF(numFeatures = 100)\n",
    "    # Each email is split into words, and each word is mapped to one feature.\n",
    "    spamFeatures = spam.map(lambda email: tf.transform(email.split(\" \")))\n",
    "    hamFeatures = ham.map(lambda email: tf.transform(email.split(\" \")))\n",
    "\n",
    "    # Create LabeledPoint datasets for positive (spam) and negative (ham) examples.\n",
    "    positiveExamples = spamFeatures.map(lambda features: LabeledPoint(1, features))\n",
    "    negativeExamples = hamFeatures.map(lambda features: LabeledPoint(0, features))\n",
    "    training_data = positiveExamples.union(negativeExamples)\n",
    "    training_data.cache() # Cache data since Logistic Regression is an iterative algorithm.\n",
    "\n",
    "    # Run Logistic Regression using the SGD optimizer.\n",
    "    # regParam is model regularization, which can make models more robust.\n",
    "    model = LogisticRegressionWithSGD.train(training_data)\n",
    "\n",
    "    # Test on a positive example (spam) and a negative one (ham).\n",
    "    # First apply the same HashingTF feature transformation used on the training data.\n",
    "    posTestExample = tf.transform(\"O M G GET cheap stuff by sending money to ...\".split(\" \"))\n",
    "    negTestExample = tf.transform(\"Hi Dad, I started studying Spark the other ...\".split(\" \"))\n",
    "\n",
    "    # Now use the learned model to predict spam/ham for new emails.\n",
    "    print \"Prediction for positive test example: %g\" % model.predict(posTestExample)\n",
    "    print \"Prediction for negative test example: %g\" % model.predict(negTestExample)\n",
    "\n",
    "    sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
